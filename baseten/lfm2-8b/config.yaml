model_name: lfm2-8b
base_image:
  image: vllm/vllm-openai:v0.12.0
model_metadata:
  engine_args:
    model: LiquidAI/LFM2-8B-A1B
    tensor_parallel_size: 1
    dtype: bfloat16
    gpu_memory_utilization: 0.6
    max_model_len: 32768
    max_num_seqs: 600
    compilation_config: '{"cudagraph_mode": "FULL_AND_PIECEWISE"}'
  example_model_input:
    messages:
      - role: user
        content: "What is the melting temperature of silver?"
    max_tokens: 100
    temperature: 0
docker_server:
  start_command: sh -c "vllm serve LiquidAI/LFM2-8B-A1B"
  readiness_endpoint: /health
  liveness_endpoint: /health
  predict_endpoint: /v1/chat/completions
  server_port: 8000
runtime:
  predict_concurrency: 50
resources:
  accelerator: H100
  use_gpu: true
  cpu: 4000m
  memory: 16Gi
environment_variables:
  HF_HUB_ENABLE_HF_TRANSFER: "1"
  VLLM_USE_V1: "1"
  HF_ACCESS_TOKEN: null
